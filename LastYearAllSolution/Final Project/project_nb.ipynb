{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# STEP 1: Download the source dataset from Kaggle to Local Disk\n",
    "\n",
    "# IMDB dataset       => https://www.kaggle.com/tomiandrep/imdb-filmid/data\n",
    "# Movie Lens dataset => https://www.kaggle.com/rounakbanik/the-movies-dataset/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# STEP 2: Copy the source datasets from Local Disk to Cloud environment [Linux Server]\n",
    "\n",
    "#scp -P 2222 \"/mnt/c/Users/sgadn/Desktop/MS - CS/MS - SEM - 4/CSP - 595 BDT/Final Project/IMDB-Movie-Data.csv\"  maria_dev@localhost:/home/maria_dev/final_project\t\t\t\t\n",
    "#scp -P 2222 \"/mnt/c/Users/sgadn/Desktop/MS - CS/MS - SEM - 4/CSP - 595 BDT/Final Project/movies_metadata.csv\"  maria_dev@localhost:/home/maria_dev/final_project\n",
    "#scp -P 2222 \"/mnt/c/Users/sgadn/Desktop/MS - CS/MS - SEM - 4/CSP - 595 BDT/Final Project/project_transform_load_script.py\"  maria_dev@localhost:/home/maria_dev/final_project\n",
    "#scp -P 2222 \"/mnt/c/Users/sgadn/Desktop/MS - CS/MS - SEM - 4/CSP - 595 BDT/Final Project/pre_processing.sh\"  maria_dev@localhost:/home/maria_dev/final_project\n",
    "#scp -P 2222 \"/mnt/c/Users/sgadn/Desktop/MS - CS/MS - SEM - 4/CSP - 595 BDT/Final Project/post_processing.sh\"  maria_dev@localhost:/home/maria_dev/final_project\n",
    "#scp -P 2222 \"/mnt/c/Users/sgadn/Desktop/MS - CS/MS - SEM - 4/CSP - 595 BDT/Final Project/project_hive_queries.hql\"  maria_dev@localhost:/home/maria_dev/final_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# STEP 3: Run Data Preprocessing script [Unix Shell Script]\n",
    "\n",
    "# Execute the following script: sh pre_processing.sh\n",
    "\n",
    "# Data Preprocessing Script:\n",
    "#  1. Clean-up to remove any source/target datasets on HDFS [Hadoop eco-system]\n",
    "#  2. Preprocess the source files to remove to remove header and \"\" characters in data for correct parsing and formatting in Hadoop-[pyspark]\n",
    "#  3. Copy the preprocessed source files from Linux Server to HDFS target directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# STEP 4: Run the Data Transformation script [pyspark script]\n",
    "\n",
    "# set the SPARK_MAJOR_VERSION parameter to 2 for enabling SparkSQL --> $ export SPARK_MAJOR_VERSION=2\n",
    "# Spawn pyspark shell --> $ pyspark\n",
    "\n",
    "# Execute the Data Transformation Pyspark script --> >>> execfile ('/home/maria_dev/final_project/project_transform_load_script.py')\n",
    "\n",
    "# Data Transformation Script:\n",
    "#  1. Define the schema for both the source files\n",
    "#  2. Using the SparkSession read the source csv files with the schema defined above\n",
    "#  3. Format the source .csv files to appropriately typecast Date, Numeric [float, double] attributes\n",
    "#  4. Sanity check for the SparkSQL dataframes\n",
    "#  5. Register the SparkSQL dataframes as tables\n",
    "#  6. Sanity check for the SparkSQL temporary tables\n",
    "#  7. Do feature extraction on Movie Lens dataset to extract year from Release_date attribute \n",
    "#  8. Perform join between the two source datasets on Title & Release_year attributes --> Use Data Fusion techniques to merge data\n",
    "#  9. For unmatched records in above step perform join on Title & Runtime attributes --> Use Data Fusion techniques to merge data\n",
    "# 10. Union/Merge the two joined dataframes from 8 & 9 into single dataframe\n",
    "# 11. Create a Hive table [Internal/Managed] of the Target table\n",
    "# 12. Write the target dataframe to .csv  format on HDFS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# STEP 5: Run Data Post-processing script [Unix Shell Script]\n",
    "\n",
    "# Execute the following script: sh post_processing.sh\n",
    "\n",
    "# Data Post-processing Script:\n",
    "#  1. Copy the target dataset from HDFS target directory to Linux Server \n",
    "#  2. Execute HQL commands from the script project_hive_queries.hql to preform analytical queries in HQL over the Hive target_table [IMDB_Movies] to gather Stylized facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# STEP 6: Copy the target dataset from Cloud environment [Linux Server] to Windows\n",
    "#         Execute R script to perform Exploratory Data Analysis, Unsupervised Learning [K-Means clustering]\n",
    "\n",
    "# scp -P 2222 maria_dev@localhost:/home/maria_dev/final_project/target_table.csv \"/mnt/c/Users/sgadn/Desktop/MS - CS/MS - SEM - 4/CSP - 595 BDT/Final Project/\"\n",
    "\n",
    "# --> Run project_analysis.R\n",
    "# R script:\n",
    "#  1. Read the target_table.csv file into R dataframe\n",
    "#  2. Plot Histograms of Release_Year, Revenue, Runtime, Ratings features\n",
    "#  3. Perform a K-Means clustering to identify clusters/region/structue in merged/target dataset"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
